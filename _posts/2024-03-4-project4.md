# Rushing to the Top 3 in PlantTraits2024 - FGVC11 Competition

## 1. Introduction

This blog aims to provide a brief report of our approach in achieving the top 3 place in the PlantTraits2024 competition in a preliminary stage. 
All results were obtained before 26 March thus, the leaderboard may change in the future.

Our Progress:

* 9th place on 3 March
* 3rd place on 26 March (after competition host updated test set due to data leakage)

The PlantTraits2024 Challenge, part of the FGVC11 workshop at CVPR 2024, provides 30,000 labeled images and also integrates ancillary geodata for predicting plant traits such as height, seed mass, leaf size, etc.

This report introduces our neural network model and data preprocessing methods, omitting numerous technical details for the purpose of simplicity.

## 2. Overview of Our Method

Our model comprises two main parts: a CLIP image encoder for feature extraction and a shallow MLP that combines these features with ancillary geodata for predictions.

<img width="480" alt="image" src="https://github.com/ywugwu/ywugwu.github.io/blob/main/_posts/imgs/Presentation1.png?raw=True">

To improve our ranking, we applied several strategies common in Kaggle competitions, including:

* Out-of-fold (OOF) Prediction
* Ensemble/Blending
* Postprocessing
* Extensive feature engineering and data preprocessing

One impactful data preprocessing technique we employed is the log transformation:


### Log Transformation

We found that three labels (X18, X26, X3112) from a total of six do not follow a normal distribution:

<img width="640" alt="image" src="https://github.com/ywugwu/ywugwu.github.io/assets/128890731/a5d2a361-6bf1-4b68-a3e6-9db1fc5e6f65">

Similarly, many ancillary features deviate from normal distribution:

<img width="640" alt="image" src="https://github.com/ywugwu/ywugwu.github.io/assets/128890731/c4c6bd60-83cb-4f2d-943c-d96a6c049e9c">

This really hinders our model's performance because many neural network components are designed based on the assumption that the data obeys normal distribution.

To address this, we applied a logarithmic transformation to non-normally distributed features,  **Namely, for each feature $x$ in the raw data that we identify as not normally distributed, we convert it to $\log(x)$.**

This trick significantly improves those distribution towards normality, followed by standardization:

<img width="640" alt="image" src="https://github.com/ywugwu/ywugwu.github.io/assets/128890731/d2281971-f5f2-42bf-958e-f47c65f89a98">

<img width="640" alt="image" src="https://github.com/ywugwu/ywugwu.github.io/assets/128890731/58e73cfc-c8c4-449f-a275-3d8935cfcc21">

## Results


**By 4 March, our approach achieved 9th place out of about 150 teams.**

<img width="640" alt="image" src="https://github.com/ywugwu/ywugwu.github.io/assets/128890731/4927c11a-fb6b-4a27-9ec3-f53fa213f108">

**By 26 March, the competition host updated the test set due to data leakage. Our approach achieved 3rd place after refreshing the leaderboard.**

<img width="640" alt="image" src="https://github.com/ywugwu/ywugwu.github.io/assets/128890731/eafb5751-1293-42f1-a5ed-b97ec53134bb">




