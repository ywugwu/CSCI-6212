# Rush to Top 3 in PlantTraits2024 - FGVC11 Competition

## 1. Introduction

This blog aims to provide a brief report of our approach in achieving the top 3 place in the PlantTraits2024 competition in a preliminary stage. All results were obtained before 26 March thus the leaderboard can change in the future.

Our Progress:

* 9th place, 3 March
* 3rd place, 26 March (after competition host updated test set)

The PlantTraits2024 Challenge, part of the FGVC11 workshop at CVPR 2024, provides 30,000 labeled images and also integrates ancillary geodata for predicting plant traits such as height, seed mass, leaf size, etc.

In this report, we will briefly introduce our proposed neural network model and some data preprocessing methods. Many technical details are omitted in this report for the purpose of simplicity.

## 2. Overview of Our Method

Our model mainly consists of two parts: 1. utilizing a CLIP image encoder to extract image features, and 2. Concatenating the image feature with ancillary geodata and utilizing a shallow multilayer perceptron (MLP) to make final prediction.

<img width="480" alt="image" src="https://github.com/ywugwu/ywugwu.github.io/blob/main/_posts/imgs/Presentation1.png?raw=True">

To boost our ranking, we also employ some techniques that are commonly used in Kaggle competitions such as:

* Out-of-fold (OOF) Prediction
* Ensemble/Blending
* Postprocessing
* A lot of feature engineering and data preprocessing

We briefly introduce one data preprocessing technique we used that has the most significant impact:


### Log Transformation

We observe that three labels (X18, X26, X3112) out of the total of 6 labels do not obey normal distribution: 

<img width="640" alt="image" src="https://github.com/ywugwu/ywugwu.github.io/assets/128890731/a5d2a361-6bf1-4b68-a3e6-9db1fc5e6f65">


We also observe that a lot of features in the ancillary features do not obey normal distribution:

<img width="640" alt="image" src="https://github.com/ywugwu/ywugwu.github.io/assets/128890731/c4c6bd60-83cb-4f2d-943c-d96a6c049e9c">

This really hinders our model's performance because many neural network components are designed based on the assumption that the data obeys normal distribution.

Thus, we apply a log trick to those features. **Namely, for each feature $x$ in the raw data that we identify as not normally distributed, we convert it to $\log(x)$.**

Then, they are distributed more normally (we also applied standarization):

<img width="640" alt="image" src="https://github.com/ywugwu/ywugwu.github.io/assets/128890731/d2281971-f5f2-42bf-958e-f47c65f89a98">

<img width="640" alt="image" src="https://github.com/ywugwu/ywugwu.github.io/assets/128890731/58e73cfc-c8c4-449f-a275-3d8935cfcc21">

## Results


**By 4 March, our approach achieved 9th place out of about 150 teams.**

<img width="640" alt="image" src="https://github.com/ywugwu/ywugwu.github.io/assets/128890731/4927c11a-fb6b-4a27-9ec3-f53fa213f108">

**By 26 March, the competition host updated the test set due to data leakage. And our approach achieved 3rd place after refreshing the leaderboard.**

<img width="640" alt="image" src="https://github.com/ywugwu/ywugwu.github.io/assets/128890731/eafb5751-1293-42f1-a5ed-b97ec53134bb">




