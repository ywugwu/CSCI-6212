# Towareds Adversarial Attack to GPT4's Vision Models

## 1. Introduction

This report aims to explore the creation of adversarial examples targeting GPT-4's vision capabilities. We utilize the Projected Gradient Descent (PGD) attack on an ensemble of CLIP models to conduct a black-box attack against GPT-4. The objective is to understand how adversarial attacks can affect GPT-4's image recognition and interpretation abilities.

## 2. Background

### 2.1. GPT-4's Vision Model
- Brief overview of GPT-4's vision capabilities.
- Previous findings on its limitations (refer to your first project).

### 2.2. Adversarial Attacks in AI
- Introduction to adversarial attacks.
- Explanation of why they are important in AI security.

### 2.3. Projected Gradient Descent (PGD)
- Explanation of PGD and its relevance to adversarial attacks.
- Rationale for choosing PGD for this study.

## 3. Methodology

### 3.1. Ensemble of CLIP Models
- Description of the CLIP models used.
- Explanation of why an ensemble approach was chosen.

### 3.2. Experiment Setup
- Detailed description of the experimental setup.
- Parameters used for the PGD attack.

### 3.3. Targeting GPT-4
- Approach to applying the generated adversarial examples on GPT-4.
- Explanation of black-box attack methodology.

## 4. Results

### 4.1. Generation of Adversarial Examples
- [Fill in details and observations regarding the adversarial examples generated.]

### 4.2. Impact on GPT-4
- [Fill in how GPT-4 responded to the adversarial examples.]
- [Include comparative analysis, if applicable.]

## 5. Discussion

- Analysis of the results.
- Insights on how adversarial attacks affect GPT-4's vision model.
- Implications of these findings for AI security.

## 6. Conclusion

- Summary of key findings.
- Potential areas for future research.


## References

- [1] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., & Vladu, A. (2017). *Towards Deep Learning Models Resistant to Adversarial Attacks*. arXiv preprint arXiv:1706.06083 [cs.LG]. Available at: [https://arxiv.org/abs/1706.06083](https://arxiv.org/abs/1706.06083)


- [2] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). *Learning Transferable Visual Models From Natural Language Supervision*. arXiv preprint arXiv:2103.00020 [cs.CV]. Available at: [https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)
