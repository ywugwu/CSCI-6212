# Towards Adversarial Attack to GPT4's Vision Models

## 1. Introduction

This report aims to explore the creation of adversarial examples targeting GPT-4's vision capabilities. We utilize the Projected Gradient Descent (PGD) attack on an ensemble of CLIP models to conduct a black-box attack against GPT-4. The objective is to understand how adversarial attacks can affect GPT-4's image recognition and interpretation abilities.

Our code is available and reproducible at https://colab.research.google.com/drive/178RTd0mkbCTDcmL3_Ud-vvuUJH0T6XAX?usp=sharing

## 2. Background
### 2.1. GPT-4's Vision Model
GPT-4, developed by OpenAI, integrates language and vision capabilities, enabling it to understand and generate responses based on textual and visual inputs. Its vision model is trained on a diverse set of images and texts, allowing for multimodal context comprehension.

### 2.2. Adversarial Attacks in AI
Adversarial attacks [1] involve manipulating input data to fool AI models into making incorrect predictions or classifications. These attacks expose vulnerabilities in AI systems and are crucial for assessing model robustness.

### 2.3. Fast Gradient Sign Method (FGSM)
FGSM [1] is a technique to create adversarial examples by applying small but intentional perturbations to input data. These perturbations are designed to cause incorrect model predictions, testing the model's resilience to subtle input changes.

## 3. Methodology
### 3.1. Ensemble of CLIP Models
The study uses an ensemble of CLIP models [2] to serve as the target models for we hope the GPT4's vision models are more similar to them than other open-source models.

### 3.2. Iterative FGSM Approach
We apply an iterative version of FGSM, gradually adjusting the input images based on gradient information. This method ensures more precise control over the perturbations, increasing the likelihood of successful adversarial attacks.

### 3.3. Black-box Attack Strategy
The black-box attack simulates a real-world scenario where the attacker has no internal knowledge of the target model. We generate adversarial examples using the CLIP ensemble and test them on GPT-4, analyzing its response to these manipulated inputs.


## 4. Results

### 4.1. Vanilla Images

We use an [cat image](https://www.alleycat.org/wp-content/uploads/2019/03/FELV-cat.jpg) as the source image and an [dog image](https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/Labrador_Retriever_portrait.jpg/1200px-Labrador_Retriever_portrait.jpg) as the target image.

<div align="center">
    <img src="https://www.alleycat.org/wp-content/uploads/2019/03/FELV-cat.jpg" width="33%">
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/Labrador_Retriever_portrait.jpg/1200px-Labrador_Retriever_portrait.jpg" width="33%">
    <br>
    <em>The cat image and the dog image we use.</em>
</div>

### 4.2. A  Adversarial Example (Our Upper Bound)

### 4.3. A Strong Adversarial Example (Our Upper Bound)

<div align="center">
    <img src="https://github.com/ywugwu/ywugwu.github.io/blob/main/_posts/imgs2/cat_to_dog.png?raw=True" width="45%">
    <br>
    <em> A strong perturbation that can pass GPT4's recognition.
</div>


### 4.2. Impact on GPT-4
- [Fill in how GPT-4 responded to the adversarial examples.]
- [Include comparative analysis, if applicable.]

## 5. Discussion

- Analysis of the results.
- Insights on how adversarial attacks affect GPT-4's vision model.
- Implications of these findings for AI security.

## 6. Conclusion

- Summary of key findings.
- Potential areas for future research.


## References

- [1] Kurakin, A., Goodfellow, I., & Bengio, S. (2016). *Adversarial Machine Learning at Scale*. arXiv preprint arXiv:1611.01236 [cs.LG]. Available at: [https://arxiv.org/abs/1611.01236](https://arxiv.org/abs/1611.01236)

- [2] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). *Learning Transferable Visual Models From Natural Language Supervision*. arXiv preprint arXiv:2103.00020 [cs.CV]. Available at: [https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)


