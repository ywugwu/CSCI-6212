# Towards Adversarial Attack to GPT4's Vision Models

## 1. Introduction

This report aims to explore the creation of adversarial examples targeting GPT-4's vision capabilities. We utilize the Fast Gradient Sign Method (FGSM) attack on an ensemble of CLIP models to conduct a black-box attack against GPT-4. The objective is to understand how adversarial attacks can affect GPT-4's image recognition and interpretation abilities.

Our code is available and reproducible at https://colab.research.google.com/drive/178RTd0mkbCTDcmL3_Ud-vvuUJH0T6XAX?usp=sharing

## 2. Background and Methodology

GPT-4, developed by OpenAI, combines language and vision capabilities, trained on diverse multimodal data. Adversarial attacks [1], crucial for testing AI robustness, involve manipulating inputs to mislead models. We utilize the Fast Gradient Sign Method (FGSM) [1] for creating adversarial examples, introducing small perturbations to images, challenging GPT-4's prediction accuracy.

Our methodology involves an ensemble of CLIP models [2], targeting GPT-4's vision model. We employ an iterative FGSM approach for precise control over image perturbations. The study adopts a black-box attack strategy, generating adversarial examples with CLIP and testing their impact on GPT-4, to analyze its response and resilience against such manipulations.


## 3. Results

### 3.1. Vanilla Images

We use an [cat image](https://www.alleycat.org/wp-content/uploads/2019/03/FELV-cat.jpg) as the source image and an [dog image](https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/Labrador_Retriever_portrait.jpg/1200px-Labrador_Retriever_portrait.jpg) as the target image.

<div align="center">
    <img src="https://www.alleycat.org/wp-content/uploads/2019/03/FELV-cat.jpg" width="33%">
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/Labrador_Retriever_portrait.jpg/1200px-Labrador_Retriever_portrait.jpg" width="33%">
    <br>
    <em>The cat image and the dog image we use.</em>
</div>

### 3.2. A  Adversarial Example (Our Upper Bound)

### 3.3. A Strong Adversarial Example (Our Upper Bound)

<div align="center">
    <img src="https://github.com/ywugwu/ywugwu.github.io/blob/main/_posts/imgs2/cat_to_dog.png?raw=True" width="45%">
    <br>
    <em> A strong perturbation that can pass GPT4's recognition. </em>
</div>


### 3.4. Impact on GPT-4
- [Fill in how GPT-4 responded to the adversarial examples.]
- [Include comparative analysis, if applicable.]


## 4. Conclusion

- Summary of key findings.
- Potential areas for future research.


## References

- [1] Kurakin, A., Goodfellow, I., & Bengio, S. (2016). *Adversarial Machine Learning at Scale*. arXiv preprint arXiv:1611.01236 [cs.LG]. Available at: [https://arxiv.org/abs/1611.01236](https://arxiv.org/abs/1611.01236)

- [2] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). *Learning Transferable Visual Models From Natural Language Supervision*. arXiv preprint arXiv:2103.00020 [cs.CV]. Available at: [https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)


